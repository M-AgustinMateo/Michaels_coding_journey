{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28713db3-3bd2-4425-8720-0d0a131d9375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Model evaluation and implementation '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Model evaluation and implementation '''\n",
    "\n",
    "# compare model performance with classification_report()\n",
    "# macro avg is assessed to get an understanding of model performance --> macro avg uses default/nondefault F1 scores\n",
    "# roc and auc used as well\n",
    "\n",
    "# we assess the calibration of the models\n",
    "# from sklearn.calibration import calibration_curve\n",
    "# calibration_curve(y_test, probabilities_of_default, n_bins = 5) \n",
    "\n",
    "# plot calibration curve\n",
    "# plt.plot(mean_predicted_value, fraction_of_positives, label=\"%s\" % \"Example Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b02941b-44d2-4152-9732-dada462822e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the logistic regression classification report\n",
    "#  target_names = ['Non-Default', 'Default']\n",
    "#  print(classification_report(y_test, preds_df_lr['loan_status'], target_names=target_names))\n",
    "\n",
    "# Print the gradient boosted tree classification report\n",
    "#  print(classification_report(y_test, preds_df_gbt['loan_status'], target_names=target_names))\n",
    "\n",
    "# Print the default F-1 scores for the logistic regression\n",
    "#  print(precision_recall_fscore_support(y_test,preds_df_lr['loan_status'], average = 'macro')[2])\n",
    "\n",
    "# Print the default F-1 scores for the gradient boosted tree\n",
    "#  print(precision_recall_fscore_support(y_test,preds_df_gbt['loan_status'], average = 'macro')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3384cf8-8278-4cb8-92ca-6326ae4f0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC chart components for logistic regression and gbt\n",
    "#  fallout_lr, sensitivity_lr, thresholds_lr = roc_curve(y_test, clf_logistic_preds)\n",
    "#  fallout_gbt, sensitivity_gbt, thresholds_gbt = roc_curve(y_test, clf_gbt_preds)\n",
    "\n",
    "# ROC Chart with both\n",
    "#  plt.plot(fallout_lr, sensitivity_lr, color = 'blue', label='%s' % 'Logistic Regression')\n",
    "#  plt.plot(fallout_gbt, sensitivity_gbt, color = 'green', label='%s' % 'GBT')\n",
    "#  plt.plot([0, 1], [0, 1], linestyle='--', label='%s' % 'Random Prediction')\n",
    "#  plt.title(\"ROC Chart for LR and GBT on the Probability of Default\")\n",
    "#  plt.xlabel('Fall-out')\n",
    "#  plt.ylabel('Sensitivity')\n",
    "#  plt.legend()\n",
    "#  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91b528c9-f788-46e4-8a4a-81e146197552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating calibration curve for the two models\n",
    "#  plt.plot([0, 1], [0, 1], 'k:', label='Perfectly calibrated')    \n",
    "#  plt.plot(mean_pred_val_lr, frac_of_pos_lr,\n",
    "#          's-', label='%s' % 'Logistic Regression')\n",
    "#  plt.plot(____, ____,\n",
    "#           's-', label='%s' % ____)\n",
    "#  plt.ylabel('Fraction of positives')\n",
    "#  plt.xlabel('Average Predicted Probability')\n",
    "#  plt.legend()\n",
    "#  plt.title('Calibration Curve')\n",
    "#  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "693b65af-9ae0-42fd-8aa0-9d37005cb545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' calculating threshold value for a given acceptance rate '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' calculating threshold value for a given acceptance rate '''\n",
    "\n",
    "#  import numpy as np\n",
    "#  threshold = np.quantile(prob_default, 0.85) --> determined the new threshold based on the model's prob_default\n",
    "#  preds_df['loan_status'] = preds_df['prob_default'].apply(lambda x: 1 if x > 0.804 else 0) --> reassigns the loan_status based on new threshold\n",
    "\n",
    "# Bad rate = Accepted defaults/ total accepted loans\n",
    "# np.sum(accepted_loans['true_loan_status']) / accepted_loans['true_loan_status'].count() \n",
    "\n",
    "# Check the statistics of the probabilities of default\n",
    "#  print(test_pred_df['prob_default'].describe())\n",
    "\n",
    "# Calculate the threshold for a 85% acceptance rate\n",
    "#  threshold_85 = np.quantile(test_pred_df['prob_default'], 0.85)\n",
    "\n",
    "# Apply acceptance rate threshold\n",
    "#  test_pred_df['pred_loan_status'] = test_pred_df['prob_default'].apply(lambda x: 1 if x > threshold_85 else 0)\n",
    "\n",
    "# Print the counts of loan status after the threshold\n",
    "#  print(test_pred_df['pred_loan_status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88837fac-9b35-4cc2-83a6-12fce688466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predicted probabilities of default\n",
    "#  plt.hist(clf_gbt_preds, color = 'blue', bins = 40)\n",
    "\n",
    "# Calculate the threshold with quantile\n",
    "#  threshold = np.quantile(clf_gbt_preds, 0.85)\n",
    "\n",
    "# Add a reference line to the plot for the threshold\n",
    "#  plt.axvline(x = threshold, color = 'red')\n",
    "#  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
